{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import numpy as np\n",
    "import panopticapi\n",
    "import requests\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms as transforms\n",
    "from panopticapi.utils import id2rgb, rgb2id\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet50\n",
    "from tqdm import tqdm\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "CLASSES = [\n",
    "    \"N/A\",\n",
    "    \"person\",\n",
    "    \"bicycle\",\n",
    "    \"car\",\n",
    "    \"motorcycle\",\n",
    "    \"airplane\",\n",
    "    \"bus\",\n",
    "    \"train\",\n",
    "    \"truck\",\n",
    "    \"boat\",\n",
    "    \"traffic light\",\n",
    "    \"fire hydrant\",\n",
    "    \"N/A\",\n",
    "    \"stop sign\",\n",
    "    \"parking meter\",\n",
    "    \"bench\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"dog\",\n",
    "    \"horse\",\n",
    "    \"sheep\",\n",
    "    \"cow\",\n",
    "    \"elephant\",\n",
    "    \"bear\",\n",
    "    \"zebra\",\n",
    "    \"giraffe\",\n",
    "    \"N/A\",\n",
    "    \"backpack\",\n",
    "    \"umbrella\",\n",
    "    \"N/A\",\n",
    "    \"N/A\",\n",
    "    \"handbag\",\n",
    "    \"tie\",\n",
    "    \"suitcase\",\n",
    "    \"frisbee\",\n",
    "    \"skis\",\n",
    "    \"snowboard\",\n",
    "    \"sports ball\",\n",
    "    \"kite\",\n",
    "    \"baseball bat\",\n",
    "    \"baseball glove\",\n",
    "    \"skateboard\",\n",
    "    \"surfboard\",\n",
    "    \"tennis racket\",\n",
    "    \"bottle\",\n",
    "    \"N/A\",\n",
    "    \"wine glass\",\n",
    "    \"cup\",\n",
    "    \"fork\",\n",
    "    \"knife\",\n",
    "    \"spoon\",\n",
    "    \"bowl\",\n",
    "    \"banana\",\n",
    "    \"apple\",\n",
    "    \"sandwich\",\n",
    "    \"orange\",\n",
    "    \"broccoli\",\n",
    "    \"carrot\",\n",
    "    \"hot dog\",\n",
    "    \"pizza\",\n",
    "    \"donut\",\n",
    "    \"cake\",\n",
    "    \"chair\",\n",
    "    \"couch\",\n",
    "    \"potted plant\",\n",
    "    \"bed\",\n",
    "    \"N/A\",\n",
    "    \"dining table\",\n",
    "    \"N/A\",\n",
    "    \"N/A\",\n",
    "    \"toilet\",\n",
    "    \"N/A\",\n",
    "    \"tv\",\n",
    "    \"laptop\",\n",
    "    \"mouse\",\n",
    "    \"remote\",\n",
    "    \"keyboard\",\n",
    "    \"cell phone\",\n",
    "    \"microwave\",\n",
    "    \"oven\",\n",
    "    \"toaster\",\n",
    "    \"sink\",\n",
    "    \"refrigerator\",\n",
    "    \"N/A\",\n",
    "    \"book\",\n",
    "    \"clock\",\n",
    "    \"vase\",\n",
    "    \"scissors\",\n",
    "    \"teddy bear\",\n",
    "    \"hair drier\",\n",
    "    \"toothbrush\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "        self.files = [x for x in self.root_dir.glob(\"**/*\") if x.is_file()]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        img_path = self.files[idx]\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        width = image.width\n",
    "        height = image.height\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return {\"image\": image, \"path\": img_path, \"width\": width, \"height\": height}\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((800, 800)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "def box_cxcywh_to_square_xyxy(x, width, height):\n",
    "    x_c, y_c, w, h = x\n",
    "    h = max(h, w * width / height)\n",
    "    w = max(w, h * height / width)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h), (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.FloatTensor(b)\n",
    "\n",
    "\n",
    "def rescale_bboxes(out_bbox, width, height):\n",
    "    b = box_cxcywh_to_square_xyxy(out_bbox, width, height)\n",
    "    b = b * torch.tensor([width, height, width, height], dtype=torch.float32)\n",
    "    return b\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "    images = torch.stack([elem[\"image\"] for elem in batch])\n",
    "    paths = [elem[\"path\"] for elem in batch]\n",
    "    widths = [elem[\"width\"] for elem in batch]\n",
    "    heights = [elem[\"height\"] for elem in batch]\n",
    "    return {\"images\": images, \"paths\": paths, \"widths\": widths, \"heights\": heights}\n",
    "\n",
    "\n",
    "bird_index = CLASSES.index(\"bird\")\n",
    "\n",
    "dataset = FullImageDataset(\"bird_dataset/train_images\", transform=transform)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_batch,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = (torch.hub.load(\"facebookresearch/detr\", \"detr_resnet50\", pretrained=True),)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm(dataloader, unit=\"batch\") as vepoch:\n",
    "    model.eval()\n",
    "    for batch in vepoch:\n",
    "        images = batch[\"images\"]\n",
    "        paths = batch[\"paths\"]\n",
    "        widths = batch[\"widths\"]\n",
    "        heights = batch[\"heights\"]\n",
    "\n",
    "        outputs = model(images)\n",
    "        for pred_boxes, pred_logits, path, width, height in zip(\n",
    "            outputs[\"pred_boxes\"], outputs[\"pred_logits\"], paths, widths, heights\n",
    "        ):\n",
    "            probas = pred_logits.softmax(-1)[:-1]\n",
    "            keep = probas[bird_index] > 0.7\n",
    "\n",
    "            im = Image.open(path).convert(\"RGB\")\n",
    "            parts = list(path.parts)\n",
    "            parts[1] += \"_cropped\"\n",
    "            cropped_path = Path(*parts)\n",
    "            cropped_path.parents[0].mkdir(parents=True, exist_ok=True)\n",
    "            if torch.sum(keep) > 0:\n",
    "                largest_box_index = np.argmin(\n",
    "                    [(box[2] * box[3]).item() for box in pred_boxes[keep]]\n",
    "                )\n",
    "                bboxes_scaled = rescale_bboxes(\n",
    "                    pred_boxes[keep][largest_box_index], width, height\n",
    "                )\n",
    "                im = im.crop(bboxes_scaled.tolist())\n",
    "            else:\n",
    "                print(f\"Image at {path} not cropped\")\n",
    "            im.save(cropped_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images with problem on train:\n",
    "\n",
    "* `004.Groove_billed_Ani/Groove_Billed_Ani_0068_1538.jpg`\n",
    "* `030.Fish_Crow/Fish_Crow_0024_26064.jpg`\n",
    "* `028.Brown_Creeper/Brown_Creeper_0111_24590.jpg`\n",
    "\n",
    "Images with problem on validation:\n",
    "\n",
    "* `026.Bronzed_Cowbird/Bronzed_Cowbird_0002_796244.jpg`\n",
    "\n",
    "Image with problem on test:\n",
    "\n",
    "* `mistery_category/0247efd7b9d47d036bb4390202a13e69.jpg`\n",
    "* `mistery_category/8ede0bc5a4976385dcfe6e38feaf90c2.jpg`\n",
    "* `mistery_category/15d6c5d42688cbc390e9ba241e93b941.jpg`\n",
    "* `mistery_category/0ab685b1515b7d4c76691e8373a65f47.jpg`\n",
    "* `mistery_category/4836bedeec2a2617ee33922ebf89995a.jpg`\n",
    "* `mistery_category/8367922ad8b74c6047ea82f4d527ce04.jpg`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/matias/.cache/torch/hub/facebookresearch_detr_main\n",
      "/home/matias/.pyenv/versions/3.9.8/envs/MVA/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/matias/.pyenv/versions/3.9.8/envs/MVA/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DETRsegm(\n",
       "  (detr): DETR(\n",
       "    (transformer): Transformer(\n",
       "      (encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (3): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (4): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (5): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (decoder): TransformerDecoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): TransformerDecoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (multihead_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): TransformerDecoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (multihead_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): TransformerDecoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (multihead_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (3): TransformerDecoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (multihead_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (4): TransformerDecoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (multihead_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (5): TransformerDecoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (multihead_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (class_embed): Linear(in_features=256, out_features=251, bias=True)\n",
       "    (bbox_embed): MLP(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (query_embed): Embedding(100, 256)\n",
       "    (input_proj): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (backbone): Joiner(\n",
       "      (0): Backbone(\n",
       "        (body): IntermediateLayerGetter(\n",
       "          (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "          (layer1): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): FrozenBatchNorm2d()\n",
       "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): FrozenBatchNorm2d()\n",
       "              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): FrozenBatchNorm2d()\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): FrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): FrozenBatchNorm2d()\n",
       "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): FrozenBatchNorm2d()\n",
       "              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): FrozenBatchNorm2d()\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Bottleneck(\n",
       "              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): FrozenBatchNorm2d()\n",
       "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): FrozenBatchNorm2d()\n",
       "              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): FrozenBatchNorm2d()\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (layer2): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): FrozenBatchNorm2d()\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn2): FrozenBatchNorm2d()\n",
       "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): FrozenBatchNorm2d()\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (1): FrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): FrozenBatchNorm2d()\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): FrozenBatchNorm2d()\n",
       "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): FrozenBatchNorm2d()\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Bottleneck(\n",
       "              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): FrozenBatchNorm2d()\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): FrozenBatchNorm2d()\n",
       "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): FrozenBatchNorm2d()\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "            (3): Bottleneck(\n",
       "              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): FrozenBatchNorm2d()\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): FrozenBatchNorm2d()\n",
       "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): FrozenBatchNorm2d()\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (layer3): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): FrozenBatchNorm2d()\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn2): FrozenBatchNorm2d()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): FrozenBatchNorm2d()\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (1): FrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): FrozenBatchNorm2d()\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): FrozenBatchNorm2d()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): FrozenBatchNorm2d()\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): FrozenBatchNorm2d()\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): FrozenBatchNorm2d()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): FrozenBatchNorm2d()\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "            (3): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): FrozenBatchNorm2d()\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): FrozenBatchNorm2d()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): FrozenBatchNorm2d()\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "            (4): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): FrozenBatchNorm2d()\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): FrozenBatchNorm2d()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): FrozenBatchNorm2d()\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "            (5): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): FrozenBatchNorm2d()\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): FrozenBatchNorm2d()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): FrozenBatchNorm2d()\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "            (6): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): FrozenBatchNorm2d()\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): FrozenBatchNorm2d()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): FrozenBatchNorm2d()\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "            (7): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): FrozenBatchNorm2d()\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): FrozenBatchNorm2d()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): FrozenBatchNorm2d()\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "            (8): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): FrozenBatchNorm2d()\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): FrozenBatchNorm2d()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): FrozenBatchNorm2d()\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "            (9): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): FrozenBatchNorm2d()\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): FrozenBatchNorm2d()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): FrozenBatchNorm2d()\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "            (10): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): FrozenBatchNorm2d()\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): FrozenBatchNorm2d()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): FrozenBatchNorm2d()\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "            (11): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): FrozenBatchNorm2d()\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): FrozenBatchNorm2d()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): FrozenBatchNorm2d()\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "            (12): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): FrozenBatchNorm2d()\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): FrozenBatchNorm2d()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): FrozenBatchNorm2d()\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "            (13): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): FrozenBatchNorm2d()\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): FrozenBatchNorm2d()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): FrozenBatchNorm2d()\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "            (14): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): FrozenBatchNorm2d()\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): FrozenBatchNorm2d()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): FrozenBatchNorm2d()\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "            (15): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): FrozenBatchNorm2d()\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): FrozenBatchNorm2d()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): FrozenBatchNorm2d()\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "            (16): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): FrozenBatchNorm2d()\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): FrozenBatchNorm2d()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): FrozenBatchNorm2d()\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "            (17): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): FrozenBatchNorm2d()\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): FrozenBatchNorm2d()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): FrozenBatchNorm2d()\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "            (18): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): FrozenBatchNorm2d()\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): FrozenBatchNorm2d()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): FrozenBatchNorm2d()\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "            (19): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): FrozenBatchNorm2d()\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): FrozenBatchNorm2d()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): FrozenBatchNorm2d()\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "            (20): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): FrozenBatchNorm2d()\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): FrozenBatchNorm2d()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): FrozenBatchNorm2d()\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "            (21): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): FrozenBatchNorm2d()\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): FrozenBatchNorm2d()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): FrozenBatchNorm2d()\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "            (22): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): FrozenBatchNorm2d()\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): FrozenBatchNorm2d()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): FrozenBatchNorm2d()\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (layer4): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): FrozenBatchNorm2d()\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn2): FrozenBatchNorm2d()\n",
       "              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): FrozenBatchNorm2d()\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (1): FrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): FrozenBatchNorm2d()\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): FrozenBatchNorm2d()\n",
       "              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): FrozenBatchNorm2d()\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Bottleneck(\n",
       "              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): FrozenBatchNorm2d()\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): FrozenBatchNorm2d()\n",
       "              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): FrozenBatchNorm2d()\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): PositionEmbeddingSine()\n",
       "    )\n",
       "  )\n",
       "  (bbox_attention): MHAttentionMap(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (q_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (k_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (mask_head): MaskHeadSmallConv(\n",
       "    (lay1): Conv2d(264, 264, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (gn1): GroupNorm(8, 264, eps=1e-05, affine=True)\n",
       "    (lay2): Conv2d(264, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (gn2): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "    (lay3): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (gn3): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
       "    (lay4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (gn4): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "    (lay5): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (gn5): GroupNorm(8, 16, eps=1e-05, affine=True)\n",
       "    (out_lay): Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (adapter1): Conv2d(1024, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (adapter2): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (adapter3): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, postprocessor = torch.hub.load(\n",
    "    \"facebookresearch/detr\",\n",
    "    \"detr_resnet101_panoptic\",\n",
    "    pretrained=True,\n",
    "    return_postprocessor=True,\n",
    "    num_classes=250,\n",
    ")\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1082 [00:00<?, ?batch/s]/home/matias/.cache/torch/hub/facebookresearch_detr_main/models/position_encoding.py:41: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n",
      "  0%|          | 1/1082 [00:05<1:36:48,  5.37s/batch]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLe Kernel s’est bloqué lors de l’exécution du code dans la cellule active ou une cellule précédente. Veuillez vérifier le code dans la ou les cellules pour identifier une cause possible de l’échec. Cliquez <a href='https://aka.ms/vscodeJupyterKernelCrash'>ici</a> pour plus d’informations. Pour plus d’informations, consultez Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "def compute_index_min_max(arr, axis):\n",
    "    diff_x = np.diff(arr.astype(float), axis=axis)\n",
    "    diff_x_min = np.argmin(diff_x, axis=axis)\n",
    "    x_min = np.min(diff_x_min[diff_x_min > 0])\n",
    "\n",
    "    diff_x_max = np.argmax(diff_x, axis=axis)\n",
    "    x_max = np.max(diff_x_max[diff_x_max > 0])\n",
    "    return x_min, x_max\n",
    "\n",
    "\n",
    "def compute_box_cxcywh(arr, square_width):\n",
    "    y_min, y_max = compute_index_min_max(arr, axis=0)\n",
    "    x_min, x_max = compute_index_min_max(arr, axis=1)\n",
    "    x_c = (x_max + x_min) / 2 / square_width\n",
    "    y_c = (y_max + y_min) / 2 / square_width\n",
    "    w = (x_max - x_min) / square_width\n",
    "    h = (y_max - y_min) / square_width\n",
    "    return [x_c, y_c, w, h]\n",
    "\n",
    "\n",
    "def perform_and_save_segmentation(\n",
    "    results, paths, widths, heights, square_width, pad_value, inference_transform\n",
    "):\n",
    "    for result, path, width, height in zip(results, paths, widths, heights):\n",
    "        segmentation = Image.open(io.BytesIO(result[\"png_string\"]))\n",
    "        segmentation = np.transpose(np.array(segmentation, dtype=np.uint8), (2, 0, 1))\n",
    "\n",
    "        im = np.array(inference_transform(Image.open(path).convert(\"RGB\")))\n",
    "        irrelevant_id = -1\n",
    "        for segment_info in result[\"segments_info\"]:\n",
    "            if segment_info[\"category_id\"] != 16:\n",
    "                irrelevant_id = segment_info[\"id\"]\n",
    "                mask = segmentation[0, :, :] == irrelevant_id\n",
    "                im[0:3, mask] = pad_value\n",
    "\n",
    "        if irrelevant_id != -1:\n",
    "            bbox = compute_box_cxcywh(\n",
    "                segmentation[0] == irrelevant_id, square_width\n",
    "            )\n",
    "            bbox_scaled = rescale_bboxes(bbox, width, height)\n",
    "            im = Image.fromarray(np.uint8(np.transpose(im * 255, (1, 2, 0)))).convert(\n",
    "                \"RGB\"\n",
    "            )\n",
    "            im = transforms.Resize((height, width))(im)\n",
    "            im = im.crop(bbox_scaled.tolist())\n",
    "\n",
    "            parts = list(path.parts)\n",
    "            parts[1] += \"_segmented\"\n",
    "            cropped_path = Path(*parts)\n",
    "            cropped_path.parents[0].mkdir(parents=True, exist_ok=True)\n",
    "        else:\n",
    "            im = Image.open(path).convert(\"RGB\")\n",
    "        im.save(cropped_path)\n",
    "\n",
    "\n",
    "with tqdm(dataloader, unit=\"batch\") as vepoch:\n",
    "    model.eval()\n",
    "    square_width = 800\n",
    "    pad_value = 1\n",
    "    inference_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((square_width, square_width)),\n",
    "            transforms.ToTensor(),\n",
    "        ],\n",
    "    )\n",
    "    for batch in vepoch:\n",
    "        images = batch[\"images\"]\n",
    "        paths = batch[\"paths\"]\n",
    "        widths = batch[\"widths\"]\n",
    "        heights = batch[\"heights\"]\n",
    "\n",
    "        outputs = model(images)\n",
    "        results = postprocessor(\n",
    "            outputs, torch.as_tensor(images.shape[-2:]).unsqueeze(0)\n",
    "        )\n",
    "        perform_and_save_segmentation(\n",
    "            results,\n",
    "            paths,\n",
    "            widths,\n",
    "            heights,\n",
    "            square_width,\n",
    "            pad_value,\n",
    "            inference_transform,\n",
    "        )\n",
    "    raise Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores = out[\"pred_logits\"].softmax(-1)[..., :-1].max(-1)[0]\n",
    "# threshold the confidence\n",
    "keep = scores > 0.85\n",
    "\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "\n",
    "palette = itertools.cycle(sns.color_palette())\n",
    "\n",
    "# The segmentation is stored in a special-format png\n",
    "panoptic_seg = Image.open(io.BytesIO(result[\"png_string\"]))\n",
    "panoptic_seg = numpy.array(panoptic_seg, dtype=numpy.uint8).copy()\n",
    "# We retrieve the ids corresponding to each mask\n",
    "panoptic_seg_id = rgb2id(panoptic_seg)\n",
    "\n",
    "# Finally we color each mask individually\n",
    "panoptic_seg[:, :, :] = 0\n",
    "for id in range(panoptic_seg_id.max() + 1):\n",
    "    panoptic_seg[panoptic_seg_id == id] = numpy.asarray(next(palette)) * 255\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.imshow(panoptic_seg)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 64-bit ('MVA')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1fcb9cfdd0ee07a1197753dad134990b9c6367a6635ff925d576420480df1e27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
